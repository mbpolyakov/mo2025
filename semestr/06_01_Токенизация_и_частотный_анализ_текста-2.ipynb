{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка текста\n",
        "\n",
        "1. **Токенизация** — самый первый шаг при обработке текста.\n",
        "2. **Нормализация** — приведение к одному регистру, удаляются пунктуации, исправление опечаток и т.д.\n",
        "3.\n",
        "    * **Стемминг** —  выделение псевдоосновы слова.\n",
        "    * **Лемматизация** — приведение слов к словарной (\"начальной\") форме.\n",
        "4. **Удаление стоп-слов** — слов, которые не несут никакой смысловой нагрузки (предлоги, союзы и т.п.) Список зависит от задачи!\n",
        "\n",
        "**Важно!** Не всегда нужны все этапы, все зависит от задачи."
      ],
      "metadata": {
        "id": "jXKsI-CvYaFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Токенизация\n",
        "\n",
        "#### Сколько слов в этом предложении?\n",
        "\n",
        "*На дворе трава, на траве дрова, не руби дрова на траве двора.*\n",
        "\n",
        "* 12 токенов: На, дворе, трава, на, траве, дрова, не, руби, дрова, на, траве, двора\n",
        "* 8 - 9 словоформ: Н/на, дворе, трава, траве, дрова, не, руби, двора.\n",
        "* 6  лексем: на, не, двор, трава, дрова, рубить\n",
        "\n",
        "\n",
        "### Токен и словоформа\n",
        "\n",
        "**Словоформа**  – уникальное слово из текста\n",
        "\n",
        "**Токен**  – словоформа и её позиция в тексте\n",
        "\n",
        "Объем корпуса измеряется в токенах, объем словаря — в словоформах или лексемах.\n",
        "\n",
        "### Обозначения\n",
        "$N$ = число токенов\n",
        "\n",
        "$V$ = словарь (все словоформы)\n",
        "\n",
        "$|V|$ = количество словоформ в словаре\n",
        "\n",
        "### Токен ≠ слово"
      ],
      "metadata": {
        "id": "mo7PZyyHYj7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Наивная токенизация\n",
        "\n",
        "Текст можно разбить на токены по пробелам."
      ],
      "metadata": {
        "id": "2vEToTKeZPkN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXhXRTayYAlq"
      },
      "outputs": [],
      "source": [
        "text ='''\n",
        "Театр \"Компас\".\n",
        "Безымянная звезда.\n",
        "\n",
        "Была на спектакле \"Безымянная звезда\" 31.01.2020 - смотрела впервые этот спектакль.\n",
        "Понравилась сама постановка, единственное показалось в первом действии слегка затянутый сюжет,\n",
        "а со второго действия было очень интересно.\n",
        "Спасибо актёрам за игру. Есть интерес и дальше знакомиться с остальными спектаклями театра.\n",
        "\n",
        "Ника написал(а)  07 февраля\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NQg2DumEg2Nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = text.split()\n",
        "print(tokens)\n",
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIFlIdX9Zwte",
        "outputId": "0c399635-c64e-4fb9-e16a-5b858ae974ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Театр', '\"Компас\"', 'Безымянная', 'звезда', 'Была', 'на', 'спектакле', '\"Безымянная', 'звезда\"', '31.01.2020', '-', 'смотрела', 'впервые', 'этот', 'спектакль.', 'Понравилась', 'сама', 'постановка,', 'единственное', 'показалось', 'в', 'первом', 'действии', 'слегка', 'затянутый', 'сюжет,', 'а', 'со', 'второго', 'действия', 'было', 'очень', 'интересно.', 'Спасибо', 'актёрам', 'за', 'игру.', 'Есть', 'интерес', 'и', 'дальше', 'знакомиться', 'с', 'остальными', 'спектаклями', 'театра.', 'Ника', 'написал(а)', '07', 'февраля']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Библиотека NTLK\n",
        "\n",
        "# Библиотека NLTK\n",
        "\n",
        "## Назначение и область применения\n",
        "**NLTK (Natural Language Toolkit)** — это библиотека для работы с естественным языком (Natural Language Processing, NLP) на языке Python. Она предоставляет инструменты для обработки текстов, анализа данных, токенизации, классификации, парсинга, семантического анализа и других задач, связанных с обработкой естественного языка.\n",
        "\n",
        "**Области применения:**\n",
        "- Лингвистические исследования\n",
        "- Машинное обучение и анализ текстов\n",
        "- Построение чат-ботов и диалоговых систем\n",
        "- Классификация текстов (спам/не спам, тональность и т.д.)\n",
        "- Извлечение информации и анализ данных\n",
        "- Обучение моделей для NLP\n",
        "\n",
        "## Разработчик\n",
        "NLTK была разработана **Стивеном Бёрдом** и **Эдвардом Лопером** в рамках образовательного проекта в Университете Пенсильвании. Библиотека активно развивается и поддерживается сообществом.\n",
        "\n",
        "## Основные функции\n",
        "1. **Токенизация**:\n",
        "   - Разбиение текста на слова, предложения или другие единицы.\n",
        "   ```python\n",
        "   from nltk.tokenize import word_tokenize\n",
        "   text = \"NLTK — это мощный инструмент для NLP.\"\n",
        "   tokens = word_tokenize(text)\n",
        "   ```\n",
        "\n",
        "2. **Стемминг и лемматизация**:\n",
        "   - Приведение слов к их базовой форме.\n",
        "   ```python\n",
        "   from nltk.stem import PorterStemmer\n",
        "   stemmer = PorterStemmer()\n",
        "   stemmed_word = stemmer.stem(\"running\")\n",
        "   ```\n",
        "\n",
        "3. **Частеречная разметка (POS-tagging)**:\n",
        "   - Определение частей речи для каждого слова.\n",
        "   ```python\n",
        "   from nltk import pos_tag\n",
        "   tagged_words = pos_tag(tokens)\n",
        "   ```\n",
        "\n",
        "4. **Синтаксический анализ**:\n",
        "   - Построение деревьев зависимостей.\n",
        "   ```python\n",
        "   from nltk import CFG, ChartParser\n",
        "   grammar = CFG.fromstring(\"S -> NP VP\")\n",
        "   parser = ChartParser(grammar)\n",
        "   ```\n",
        "\n",
        "5. **Классификация текстов**:\n",
        "   - Обучение моделей для классификации текстов.\n",
        "   ```python\n",
        "   from nltk.classify import NaiveBayesClassifier\n",
        "   classifier = NaiveBayesClassifier.train(train_data)\n",
        "   ```\n",
        "\n",
        "6. **Работа с корпусами текстов**:\n",
        "   - Доступ к предварительно загруженным текстовым данным.\n",
        "   ```python\n",
        "   from nltk.corpus import gutenberg\n",
        "   text = gutenberg.raw(\"shakespeare-hamlet.txt\")\n",
        "   ```\n",
        "\n",
        "7. **Извлечение именованных сущностей (NER)**:\n",
        "   - Поиск имен, организаций, дат и других сущностей.\n",
        "   ```python\n",
        "   from nltk import ne_chunk\n",
        "   entities = ne_chunk(tagged_words)\n",
        "   ```\n",
        "\n",
        "8. **Анализ тональности**:\n",
        "   - Определение эмоциональной окраски текста.\n",
        "   ```python\n",
        "   from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "   sia = SentimentIntensityAnalyzer()\n",
        "   sentiment = sia.polarity_scores(\"NLTK is amazing!\")\n",
        "   ```\n",
        "\n",
        "NLTK — это мощный инструмент для работы с текстами, который подходит как для образовательных целей, так и для профессионального использования в NLP. Она предоставляет широкий набор функций для анализа и обработки естественного языка, что делает её одной из самых популярных библиотек в этой области.\n",
        "\n",
        "Каждый из элементов, загружаемых с помощью `nltk.download()`, представляет собой набор данных, моделей или ресурсов, необходимых для выполнения определённых задач в библиотеке NLTK. Вот подробное объяснение, зачем нужны эти элементы:\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **`nltk.download('stopwords')`**\n",
        "   - **Назначение**: Загружает список стоп-слов для различных языков.\n",
        "   - **Зачем нужен**: Стоп-слова — это слова, которые часто встречаются в тексте, но не несут значимой смысловой нагрузки (например, предлоги, союзы, местоимения). Они удаляются на этапе предобработки текста, чтобы уменьшить размер данных и улучшить качество анализа.\n",
        "   - **Пример использования**:\n",
        "     ```python\n",
        "     from nltk.corpus import stopwords\n",
        "     stop_words = set(stopwords.words('english'))\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **`nltk.download('punkt')`**\n",
        "   - **Назначение**: Загружает токенизатор для разделения текста на слова и предложения.\n",
        "   - **Зачем нужен**: Токенизация — это первый шаг в обработке текста. Этот ресурс позволяет разбивать текст на отдельные слова или предложения, что необходимо для дальнейшего анализа.\n",
        "   - **Пример использования**:\n",
        "     ```python\n",
        "     from nltk.tokenize import word_tokenize\n",
        "     tokens = word_tokenize(\"This is a sample sentence.\")\n",
        "     ```\n",
        "---    \n",
        "### **`nltk.download('punkt_tab')`**\n",
        "   - это ресурс, связанный с токенизацией текста в библиотеке NLTK. Однако, в отличие от стандартного punkt, этот ресурс используется для работы с табулярными данными (например, текстами, разделёнными табуляцией или другими разделителями).\n",
        "---\n",
        "\n",
        "### 3. **`nltk.download('snowball_data')`**\n",
        "   - **Назначение**: Загружает данные для стеммера Snowball.\n",
        "   - **Зачем нужен**: Snowball — это стеммер, который используется для приведения слов к их корневой форме (например, \"running\" → \"run\"). Этот ресурс необходим для работы стеммера.\n",
        "   - **Пример использования**:\n",
        "     ```python\n",
        "     from nltk.stem import SnowballStemmer\n",
        "     stemmer = SnowballStemmer(\"english\")\n",
        "     stemmed_word = stemmer.stem(\"running\")\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **`nltk.download('perluniprops')`**\n",
        "   - **Назначение**: Загружает данные для работы с Unicode-свойствами символов.\n",
        "   - **Зачем нужен**: Этот ресурс используется для анализа текста на основе Unicode (например, определение, является ли символ буквой, цифрой или пробелом). Он полезен для задач токенизации и нормализации текста.\n",
        "   - **Пример использования**: Используется внутри NLTK для обработки текста.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **`nltk.download('universal_tagset')`**\n",
        "   - **Назначение**: Загружает универсальный набор тегов для частеречной разметки.\n",
        "   - **Зачем нужен**: Универсальный набор тегов упрощает классификацию частей речи, используя общие категории (например, \"NOUN\", \"VERB\", \"ADJ\"). Это полезно для анализа текста и сравнения результатов между разными языками.\n",
        "   - **Пример использования**:\n",
        "     ```python\n",
        "     from nltk import pos_tag\n",
        "     tagged_words = pos_tag([\"This\", \"is\", \"a\", \"test\"], tagset='universal')\n",
        "     ```\n",
        "\n",
        "---\n",
        "\n",
        "### 6. **`nltk.download('nonbreaking_prefixes')`**\n",
        "   - **Назначение**: Загружает список префиксов, которые не должны разрывать предложения.\n",
        "   - **Зачем нужен**: Этот ресурс используется для корректной токенизации предложений. Например, сокращения вроде \"Mr.\" или \"Dr.\" не должны разрывать предложение на части.\n",
        "   - **Пример использования**: Используется внутри NLTK для токенизации предложений.\n",
        "\n",
        "---\n",
        "\n",
        "### 7. **`nltk.download('wordnet')`**\n",
        "   - **Назначение**: Загружает лексическую базу данных WordNet.\n",
        "   - **Зачем нужен**: WordNet — это семантическая сеть, которая группирует слова в синонимические наборы (синсеты) и предоставляет информацию о их значениях, синонимах, антонимах и других связях. Используется для лемматизации, поиска синонимов и анализа смысла слов.\n",
        "   - **Пример использования**:\n",
        "     ```python\n",
        "     from nltk.corpus import wordnet\n",
        "     synsets = wordnet.synsets(\"dog\")\n",
        "     ```\n",
        "\n"
      ],
      "metadata": {
        "id": "CNEpwhU6Z4n2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('snowball_data')\n",
        "nltk.download('perluniprops')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('nonbreaking_prefixes')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEb25UyzZ8Fg",
        "outputId": "e32af5a7-3c9e-4269-c660-8a7ae213d2ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package snowball_data to /root/nltk_data...\n",
            "[nltk_data]   Package snowball_data is already up-to-date!\n",
            "[nltk_data] Downloading package perluniprops to /root/nltk_data...\n",
            "[nltk_data]   Package perluniprops is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize, ToktokTokenizer\n",
        "#word_tokenize — это встроенная функция NLTK для токенизации текста на слова.\n",
        "#Она использует предобученную модель punkt, которая хорошо работает для большинства задач.\n",
        "#ToktokTokenizer — это токенизатор, разработанный для работы с текстами на низкоресурсных языках\n",
        "#(например, африканские или азиатские языки).\n",
        "#Он менее сложен, чем punkt, но может быть полезен для специфических задач.\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)\n",
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ck_aqk2HhxDT",
        "outputId": "32ec2d97-f804-4ef3-f13b-9581c8785c38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Театр', '``', 'Компас', \"''\", 'Безымянная', 'звезда', 'Была', 'на', 'спектакле', '``', 'Безымянная', 'звезда', \"''\", '31.01.2020', '-', 'смотрела', 'впервые', 'этот', 'спектакль', '.', 'Понравилась', 'сама', 'постановка', ',', 'единственное', 'показалось', 'в', 'первом', 'действии', 'слегка', 'затянутый', 'сюжет', ',', 'а', 'со', 'второго', 'действия', 'было', 'очень', 'интересно', '.', 'Спасибо', 'актёрам', 'за', 'игру', '.', 'Есть', 'интерес', 'и', 'дальше', 'знакомиться', 'с', 'остальными', 'спектаклями', 'театра', '.', 'Ника', 'написал', '(', 'а', ')', '07', 'февраля']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "63"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tknzr = ToktokTokenizer()\n",
        "tokens = tknzr.tokenize(text)\n",
        "print(tokens)\n",
        "len(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0EJnMEnixEe",
        "outputId": "ee924ded-4c07-4cdb-902a-da0e8933cb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Театр', '\"', 'Компас', '\"', 'Безымянная', 'звезда', 'Была', 'на', 'спектакле', '\"', 'Безымянная', 'звезда', '\"', '31.01.2020', '-', 'смотрела', 'впервые', 'этот', 'спектакль.', 'Понравилась', 'сама', 'постановка', ',', 'единственное', 'показалось', 'в', 'первом', 'действии', 'слегка', 'затянутый', 'сюжет', ',', 'а', 'со', 'второго', 'действия', 'было', 'очень', 'интересно.', 'Спасибо', 'актёрам', 'за', 'игру.', 'Есть', 'интерес', 'и', 'дальше', 'знакомиться', 'с', 'остальными', 'спектаклями', 'театра.', 'Ника', 'написал', '(', 'а', ')', '07', 'февраля']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#В nltk есть довольно много токенизаторов:\n",
        "from nltk import tokenize\n",
        "dir(tokenize)[:16]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8SCmS0-ja78",
        "outputId": "2792255d-6f7a-4857-a302-5c56b205234f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BlanklineTokenizer',\n",
              " 'LegalitySyllableTokenizer',\n",
              " 'LineTokenizer',\n",
              " 'MWETokenizer',\n",
              " 'NLTKWordTokenizer',\n",
              " 'PunktSentenceTokenizer',\n",
              " 'PunktTokenizer',\n",
              " 'RegexpTokenizer',\n",
              " 'ReppTokenizer',\n",
              " 'SExprTokenizer',\n",
              " 'SpaceTokenizer',\n",
              " 'StanfordSegmenter',\n",
              " 'SyllableTokenizer',\n",
              " 'TabTokenizer',\n",
              " 'TextTilingTokenizer',\n",
              " 'ToktokTokenizer']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Сегментация предложений\n",
        "\n",
        "Сегментацию предложений иногда называют **сплиттингом**.\n",
        "\n",
        "Основные признаки — знаки препинания. \"?\", \"!\" как правило однозначны, проблемы возникают с \".\"  Возможное решение: бинарный классификатор для сегментации предложений. Для каждой точки \".\" определить, является ли она концом предложения или нет."
      ],
      "metadata": {
        "id": "6aA1t3QBjnEL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sents = sent_tokenize(text)\n",
        "print(len(sents))\n",
        "sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DgnMSx3joR0",
        "outputId": "79e1c0f4-aaf9-4793-a398-a079d03ce14c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nТеатр \"Компас\".',\n",
              " 'Безымянная звезда.',\n",
              " 'Была на спектакле \"Безымянная звезда\" 31.01.2020 - смотрела впервые этот спектакль.',\n",
              " 'Понравилась сама постановка, единственное показалось в первом действии слегка затянутый сюжет, \\nа со второго действия было очень интересно.',\n",
              " 'Спасибо актёрам за игру.',\n",
              " 'Есть интерес и дальше знакомиться с остальными спектаклями театра.',\n",
              " 'Ника написал(а)  07 февраля']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**rusenttokenize** — это специализированный инструмент для токенизации русскоязычных текстов на предложения. Он входит в библиотеку NLTK и предназначен для корректного разделения текста на предложения с учётом особенностей русского языка."
      ],
      "metadata": {
        "id": "lQd-doq-kOob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --user rusenttokenize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsSiU8Qpj6RD",
        "outputId": "4b637367-327d-490c-d193-f1b80fa82dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rusenttokenize in /root/.local/lib/python3.11/site-packages (0.0.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rusenttokenize import ru_sent_tokenize\n",
        "sents = ru_sent_tokenize(text)\n",
        "\n",
        "print(len(sents))\n",
        "sents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEAWTihQj3vV",
        "outputId": "76cac123-197d-4335-98fa-c71685daf831"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Театр \"Компас\".',\n",
              " 'Безымянная звезда.',\n",
              " 'Была на спектакле \"Безымянная звезда\" 31.01.2020 - смотрела впервые этот спектакль.',\n",
              " 'Понравилась сама постановка, единственное показалось в первом действии слегка затянутый сюжет, \\nа со второго действия было очень интересно.',\n",
              " 'Спасибо актёрам за игру.',\n",
              " 'Есть интерес и дальше знакомиться с остальными спектаклями театра.',\n",
              " 'Ника написал(а)  07 февраля']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Нормализация\n",
        "\n",
        "**Нормализация в NLP (Natural Language Processing)** — это процесс приведения текста к единому стандартному формату, чтобы упростить его дальнейшую обработку и анализ. Нормализация включает в себя различные этапы, такие как приведение текста к нижнему регистру, удаление лишних символов, исправление ошибок, приведение слов к их базовой форме и другие преобразования.\n",
        "\n",
        "Цель нормализации — уменьшить разнообразие форм слов и текстовых данных, чтобы улучшить качество анализа и упростить работу алгоритмов машинного обучения.\n",
        "\n",
        "1. **Уменьшение размерности данных**:\n",
        "   - Нормализация сокращает количество уникальных слов, что упрощает обработку и анализ.\n",
        "2. **Улучшение качества моделей**:\n",
        "   - Алгоритмы машинного обучения работают лучше, когда данные представлены в едином формате.\n",
        "3. **Упрощение анализа**:\n",
        "   - Нормализация делает текст более структурированным и удобным для анализа.\n",
        "4. **Сравнение текстов**:\n",
        "   - Нормализация позволяет сравнивать тексты, игнорируя незначительные различия (например, регистр, пунктуацию).\n",
        "\n",
        "## Основные этапы нормализации\n",
        "\n",
        "### 1. **Приведение к нижнему регистру**\n",
        "   - Все символы в тексте приводятся к нижнему регистру, чтобы избежать дублирования слов из-за разного регистра (например, \"Слово\" и \"слово\" будут считаться одним и тем же словом).\n",
        "   - **Пример**:\n",
        "     ```python\n",
        "     text = \"Это Пример Текста.\"\n",
        "     normalized_text = text.lower()\n",
        "     print(normalized_text)  # \"это пример текста.\"\n",
        "     ```\n",
        "\n",
        "### 2. **Удаление пунктуации и специальных символов**\n",
        "   - Удаляются знаки препинания, цифры, специальные символы и другие неалфавитные элементы, которые не несут смысловой нагрузки.\n",
        "   - **Пример**:\n",
        "     ```python\n",
        "     import re\n",
        "     text = \"Это пример текста, с пунктуацией! И цифрами: 123.\"\n",
        "     normalized_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "     print(normalized_text)  # \"Это пример текста с пунктуацией И цифрами 123\"\n",
        "     ```\n",
        "\n",
        "### 3. **Удаление стоп-слов**\n",
        "   - Удаляются стоп-слова (слова, которые не несут значимой информации, такие как предлоги, союзы, местоимения).\n",
        "   - **Пример**:\n",
        "     ```python\n",
        "     from nltk.corpus import stopwords\n",
        "     stop_words = set(stopwords.words('russian'))\n",
        "     text = \"Это пример текста с некоторыми стоп-словами.\"\n",
        "     normalized_text = \" \".join([word for word in text.split() if word not in stop_words])\n",
        "     print(normalized_text)  # \"пример текста стоп-словами.\"\n",
        "     ```\n",
        "\n",
        "### 4. **Лемматизация**\n",
        "   - Приведение слов к их базовой форме (лемме). Например, \"бежал\", \"бежать\", \"бегу\" → \"бежать\".\n",
        "   - **Пример**:\n",
        "     ```python\n",
        "     from nltk.stem import WordNetLemmatizer\n",
        "     lemmatizer = WordNetLemmatizer()\n",
        "     word = \"running\"\n",
        "     normalized_word = lemmatizer.lemmatize(word, pos='v')\n",
        "     print(normalized_word)  # \"run\"\n",
        "     ```\n",
        "\n",
        "### 5. **Стемминг**\n",
        "   - Приведение слов к их корневой форме путём отсечения окончаний. Например, \"бежал\", \"бежать\", \"бегу\" → \"беж\".\n",
        "  \n",
        "### 6. **Исправление опечаток**\n",
        "   - Автоматическое исправление ошибок в тексте (например, \"првиет\" → \"привет\").\n",
        "   - **Пример**:\n",
        "     Используются библиотеки, такие как `pyspellchecker` или `symspellpy`.\n",
        "\n",
        "### 7. **Удаление лишних пробелов**\n",
        "   - Удаление лишних пробелов и приведение текста к единому формату.\n",
        "   - **Пример**:\n",
        "     ```python\n",
        "     text = \"Это   текст  с  лишними  пробелами.\"\n",
        "     normalized_text = \" \".join(text.split())\n",
        "     print(normalized_text)  # \"Это текст с лишними пробелами.\"\n",
        "     ```\n"
      ],
      "metadata": {
        "id": "atuvEg_Bl_j9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Удаление пунктуации"
      ],
      "metadata": {
        "id": "Fb2_GAdGm7cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Удаление пунктуации и цифр\n",
        "clean_words = [word for word in tokens if word.isalpha()]\n",
        "print(clean_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVUXMWb0mqQq",
        "outputId": "5e26ba70-4ea1-402d-f0aa-bc085d3eaacd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Театр', 'Компас', 'Безымянная', 'звезда', 'Была', 'на', 'спектакле', 'Безымянная', 'звезда', 'смотрела', 'впервые', 'этот', 'Понравилась', 'сама', 'постановка', 'единственное', 'показалось', 'в', 'первом', 'действии', 'слегка', 'затянутый', 'сюжет', 'а', 'со', 'второго', 'действия', 'было', 'очень', 'Спасибо', 'актёрам', 'за', 'Есть', 'интерес', 'и', 'дальше', 'знакомиться', 'с', 'остальными', 'спектаклями', 'Ника', 'написал', 'а', 'февраля']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Преобразование регистра"
      ],
      "metadata": {
        "id": "aevUQerQnAHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_words = [w.lower() for w in clean_words if w != '']\n",
        "print(clean_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijybd4pCnBT2",
        "outputId": "dacd27b7-c154-460f-c0bb-a8ac835e5128"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['театр', 'компас', 'безымянная', 'звезда', 'была', 'на', 'спектакле', 'безымянная', 'звезда', 'смотрела', 'впервые', 'этот', 'понравилась', 'сама', 'постановка', 'единственное', 'показалось', 'в', 'первом', 'действии', 'слегка', 'затянутый', 'сюжет', 'а', 'со', 'второго', 'действия', 'было', 'очень', 'спасибо', 'актёрам', 'за', 'есть', 'интерес', 'и', 'дальше', 'знакомиться', 'с', 'остальными', 'спектаклями', 'ника', 'написал', 'а', 'февраля']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Стоп-слова\n",
        "\n",
        "**Стоп-слова** — высокочастотные слова, которые не дают нам никакой информации о конкретном тексте. Они составляют верхушку частотного списка в любом языке. Набор стоп-слов не универсален, он будет зависеть от вашей задачи!\n",
        "\n",
        "В NLTK есть готовые списки стоп-слов для многих языков."
      ],
      "metadata": {
        "id": "B6pKQZ4AnKl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "# смотрим, какие языки есть\n",
        "stopwords.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dreFcClVnK_w",
        "outputId": "d76fd825-2f6d-4eb0-9d24-eeedbf8d4263"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['albanian',\n",
              " 'arabic',\n",
              " 'azerbaijani',\n",
              " 'basque',\n",
              " 'belarusian',\n",
              " 'bengali',\n",
              " 'catalan',\n",
              " 'chinese',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'greek',\n",
              " 'hebrew',\n",
              " 'hinglish',\n",
              " 'hungarian',\n",
              " 'indonesian',\n",
              " 'italian',\n",
              " 'kazakh',\n",
              " 'nepali',\n",
              " 'norwegian',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'slovene',\n",
              " 'spanish',\n",
              " 'swedish',\n",
              " 'tajik',\n",
              " 'tamil',\n",
              " 'turkish']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sw = stopwords.words('russian')\n",
        "print(sw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8PsKJlpnO8L",
        "outputId": "0575ac96-9b3c-45f2-cedd-de092b8fff95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = [w for w in clean_words if w in sw]\n",
        "print(stop_words)\n",
        "real_words = [w for w in clean_words if w not in sw]\n",
        "print(real_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaLeJpx0nSjz",
        "outputId": "31c51594-fff5-4a88-842d-a7094cc5d619"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['была', 'на', 'этот', 'в', 'а', 'со', 'было', 'за', 'есть', 'и', 'с', 'а']\n",
            "['театр', 'компас', 'безымянная', 'звезда', 'спектакле', 'безымянная', 'звезда', 'смотрела', 'впервые', 'понравилась', 'сама', 'постановка', 'единственное', 'показалось', 'первом', 'действии', 'слегка', 'затянутый', 'сюжет', 'второго', 'действия', 'очень', 'спасибо', 'актёрам', 'интерес', 'дальше', 'знакомиться', 'остальными', 'спектаклями', 'ника', 'написал', 'февраля']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Стемминг\n",
        "\n",
        "**Стемминг** — отсечение от слова окончаний и суффиксов, чтобы оставшаяся часть, называемая stem, была одинаковой для всех грамматических форм слова. Стем необязательно совпадает с морфлогической основой слова. Одинаковый стем может получиться и не у однокоренных слов и наоборот — в этом проблема стемминга.\n",
        "\n",
        "* 1-ый вид ошибки: белый, белка, белье $\\implies$  бел\n",
        "\n",
        "* 2-ой вид ошибки: трудность, трудный $\\implies$  трудност, труд\n",
        "\n",
        "* 3-ий вид ошибки: быстрый, быстрее $\\implies$  быст, побыстрее $\\implies$  побыст\n",
        "\n",
        "Самый простой алгоритм, алгоритм Портера, состоит из 5 циклов команд, на каждом цикле – операция удаления / замены суффикса. Возможны вероятностные расширения алгоритма.\n",
        "\n",
        "### Snowball stemmer\n",
        "Улучшенный вариант стеммера Портера; в отличие от него умеет работать не только с английским текстом."
      ],
      "metadata": {
        "id": "GTvI2MUpn6b7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "SnowballStemmer.languages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhUXmDPLn7uh",
        "outputId": "7cd36fea-cea8-48d3-8d51-cc99391cc9a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [w for w in real_words if w not in sw and w != '']\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMLvGewWoG_w",
        "outputId": "20ed37c4-8c11-46fe-a129-793823e7ad8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['театр', 'компас', 'безымянная', 'звезда', 'спектакле', 'безымянная', 'звезда', 'смотрела', 'впервые', 'понравилась', 'сама', 'постановка', 'единственное', 'показалось', 'первом', 'действии', 'слегка', 'затянутый', 'сюжет', 'второго', 'действия', 'очень', 'спасибо', 'актёрам', 'интерес', 'дальше', 'знакомиться', 'остальными', 'спектаклями', 'ника', 'написал', 'февраля']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowball = SnowballStemmer(\"russian\")\n",
        "\n",
        "for w in real_words:\n",
        "    print(\"%s: %s\" % (w, snowball.stem(w)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gYO_9Ppoonp",
        "outputId": "751f3dbe-f922-4477-cdd2-622b31647dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "театр: театр\n",
            "компас: компас\n",
            "безымянная: безымя\n",
            "звезда: звезд\n",
            "спектакле: спектакл\n",
            "безымянная: безымя\n",
            "звезда: звезд\n",
            "смотрела: смотрел\n",
            "впервые: вперв\n",
            "понравилась: понрав\n",
            "сама: сам\n",
            "постановка: постановк\n",
            "единственное: единствен\n",
            "показалось: показа\n",
            "первом: перв\n",
            "действии: действ\n",
            "слегка: слегк\n",
            "затянутый: затянут\n",
            "сюжет: сюжет\n",
            "второго: втор\n",
            "действия: действ\n",
            "очень: очен\n",
            "спасибо: спасиб\n",
            "актёрам: актер\n",
            "интерес: интерес\n",
            "дальше: дальш\n",
            "знакомиться: знаком\n",
            "остальными: остальн\n",
            "спектаклями: спектакл\n",
            "ника: ник\n",
            "написал: написа\n",
            "февраля: феврал\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Частотный анализ текста\n"
      ],
      "metadata": {
        "id": "I0CJ6p8OqPwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n"
      ],
      "metadata": {
        "id": "cUTcGXQboxf1"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Подсчёт частоты слов\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# Вывод 10 самых частых слов\n",
        "print(\"10 самых частых слов:\")\n",
        "print(freq_dist.most_common(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "F9T_sfYHqcr7",
        "outputId": "1904fe2e-328e-4d27-ff33-d22ab51f57de"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokens' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-2ca8b97e654d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Подсчёт частоты слов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfreq_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Вывод 10 самых частых слов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"10 самых частых слов:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Задание:\n",
        "Проведите частотный анализ текста:\n",
        "\n",
        "Миром правят мечты. Вы скажете, что так бывает только в кино. Но не стоит забывать, что съёмки фильмов – это не только колоссальный труд, но и тоже чья-то мечта.\n",
        "\n",
        "Особенно показательным является история создания фильма «Фицкарральдо» немецкого режиссёра Вернера Херцога.\n",
        "\n",
        "Лента повествует о фанатично влюблённом в музыку и голос великого тенора Энрико Карузо этническом ирландце, обанкротившемся магнате Брайане Фицджеральде, живущем в Латинской Америке. Ради того, чтобы вживую услышать голос кумира, он проделывает путешествие в 2000 миль по Амазонке. После встречи с прекрасным он решает построить оперный театр в деревушке Икитос – самом сердце непроходимой перуанской сельвы. Но, как это зачастую и бывает, осуществлению мечты мешает финансовый вопрос. Фицкарральдо, как называют Фицджеаральда в Америке, решает снарядить экспедицию ради разработки каучуковых плантаций, никем доселе нетронутых ввиду их недоступности. Но дорога к богатству лежит через преодоление крайне опасных порогов рек и встречу с враждебно настроенными индейскими племенами.\n",
        "\n",
        "Звучит невероятно, правда? Однако главный герой фильма имел реальный прототип – «каучукового барона» Карлоса Фермина Фицкарральдо, на самом деле осуществившим сложнейший переход через смертельно опасный порог на корабле массой 28 тонн, уговорив помочь в этом индейцев.\n",
        "\n",
        "Согласитесь, впечатляет. Но и это далеко не всё. Дело в том, что преданный искусству не менее фанатично, чем его герой, режиссёр Вернер Херцог на протяжении четырёх лет создавал картину о том, на что способен человек, ведомый одной лишь мечтой. Он повторил невероятную экспедицию, собрав корабль массой 328 (!) тонн и заручившись поддержкой и согласием на съёмки вождя настоящего индейского племени. Все сцены фильма выполнялись без использования спецэффектов: режиссёр с маниакальной страстью старался убедить зрителя в реальности происходящего. Съёмки происходили в тяжелейших условиях непроходимых перуанских лесов, нетронутых цивилизацией и девственно чистых.\n",
        "\n",
        "Вот так, рассказывая о воплощении безумной мечты, совершенно невозможной и немыслимой, Херцог воплотил в жизнь свою. Он подарил нам эпических размахов воодушевляющую историю. За проведённую титаническую работу Вернер Херцог был удостоен пальмовой ветви лучшему режиссёру на престижнейшем кинофестивале в Канне в 1982 году."
      ],
      "metadata": {
        "id": "Ax9HEVPTq4KC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Загрузка необходимых ресурсов NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('snowball_data')\n",
        "\n",
        "text = \"\"\"\n",
        "Миром правят мечты. Вы скажете, что так бывает только в кино. Но не стоит забывать, что съёмки фильмов – это не только колоссальный труд, но и тоже чья-то мечта.\n",
        "\n",
        "Особенно показательным является история создания фильма «Фицкарральдо» немецкого режиссёра Вернера Херцога.\n",
        "\n",
        "Лента повествует о фанатично влюблённом в музыку и голос великого тенора Энрико Карузо этническом ирландце, обанкротившемся магнате Брайане Фицджеральде, живущем в Латинской Америке. Ради того, чтобы вживую услышать голос кумира, он проделывает путешествие в 2000 миль по Амазонке. После встречи с прекрасным он решает построить оперный театр в деревушке Икитос – самом сердце непроходимой перуанской сельвы. Но, как это зачастую и бывает, осуществлению мечты мешает финансовый вопрос. Фицкарральдо, как называют Фицджеаральда в Америке, решает снарядить экспедицию ради разработки каучуковых плантаций, никем доселе нетронутых ввиду их недоступности. Но дорога к богатству лежит через преодоление крайне опасных порогов рек и встречу с враждебно настроенными индейскими племенами.\n",
        "\n",
        "Звучит невероятно, правда? Однако главный герой фильма имел реальный прототип – «каучукового барона» Карлоса Фермина Фицкарральдо, на самом деле осуществившим сложнейший переход через смертельно опасный порог на корабле массой 28 тонн, уговорив помочь в этом индейцев.\n",
        "\n",
        "Согласитесь, впечатляет. Но и это далеко не всё. Дело в том, что преданный искусству не менее фанатично, чем его герой, режиссёр Вернер Херцог на протяжении четырёх лет создавал картину о том, на что способен человек, ведомый одной лишь мечтой. Он повторил невероятную экспедицию, собрав корабль массой 328 (!) тонн и заручившись поддержкой и согласием на съёмки вождя настоящего индейского племени. Все сцены фильма выполнялись без использования спецэффектов: режиссёр с маниакальной страстью старался убедить зрителя в реальности происходящего. Съёмки происходили в тяжелейших условиях непроходимых перуанских лесов, нетронутых цивилизацией и девственно чистых.\n",
        "\n",
        "Вот так, рассказывая о воплощении безумной мечты, совершенно невозможной и немыслимой, Херцог воплотил в жизнь свою. Он подарил нам эпических размахов воодушевляющую историю. За проведённую титаническую работу Вернер Херцог был удостоен пальмовой ветви лучшему режиссёру на престижнейшем кинофестивале в Канне в 1982 году.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BRcP7CeWr82y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8d9f0a9-f93a-40e7-ae22-c47ae99fd6e4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package snowball_data to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.lower()"
      ],
      "metadata": {
        "id": "rDWyHlmCq6Ne"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = re.sub(r'[^\\w\\s]', '', text)\n",
        "text = re.sub(r'\\d+', '', text)"
      ],
      "metadata": {
        "id": "hY3TDs49rvgK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "russian_stopwords = set(stopwords.words('russian'))\n",
        "words = [word for word in words if word not in russian_stopwords]"
      ],
      "metadata": {
        "id": "EIBnfla9rzei",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "36aeba34-a1f4-47ed-d6c4-158870d0d8d1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'words' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-68b63e71dc94>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mrussian_stopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'russian'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrussian_stopwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = SnowballStemmer('russian')\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n"
      ],
      "metadata": {
        "id": "sYwBzJc7sXnS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "ac3d66a8-c779-4728-cd24-d217812b9020"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'words' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a45d83658688>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstemmer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'russian'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstemmed_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'words' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_counts = Counter(stemmed_words)\n",
        "most_common_words = word_counts.most_common(10)\n",
        "\n",
        "print(\"\\n10 самых частых слов:\")\n",
        "for word, count in most_common_words:\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "fEFA1cNHsa_i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "3c587f41-1e93-464a-e19b-1d4659d6d954"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'stemmed_words' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-f6ec1a65c70c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mword_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmost_common_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n10 самых частых слов:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmost_common_words\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stemmed_words' is not defined"
          ]
        }
      ]
    }
  ]
}